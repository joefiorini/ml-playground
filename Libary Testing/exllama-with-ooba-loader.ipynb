{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: PYTORCH_CUDA_ALLOC_CONF=backend:cudaMallocAsync\n",
      "/home/jovyan/work/importer.py\n"
     ]
    }
   ],
   "source": [
    "%set_env PYTORCH_CUDA_ALLOC_CONF=backend:cudaMallocAsync\n",
    "\n",
    "import importer\n",
    "from exllama.lora import ExLlamaLora\n",
    "from exllama.tokenizer import ExLlamaTokenizer\n",
    "\n",
    "# from exllama_extras.model_loaders import ooba_loader\n",
    "# from exllama_extras import model_init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once upon a time, there was an island in the middle of nowhere. It had been abandoned for years and nobody knew why it existed or what secrets lay hidden within its walls.\n",
      "Verse 1:\n",
      "On this deserted isle so far from home\n",
      "Lies a mystery waiting to be known\n",
      "No one knows how long it's stood here alone\n",
      "But something about it draws us close like stone\n",
      "\n",
      "Chorus:\n",
      "We come seeking answers to questions unasked\n",
      "Hoping that our journey won't leave us masked\n",
      "For on this forgotten land we may find\n",
      "The truth behind a story left undefined\n",
      "\n",
      "Bridge:\n",
      "As we explore every corner and crevice\n",
      "Our hearts race with anticipation and persistence\n",
      "Could it be possible that legends are true?\n",
      "That magic exists just beyond view\n",
      "\n",
      "Chorus:\n",
      "We come seeking answers to questions unasked\n",
      "Hoping that our journey won't leave us masked\n"
     ]
    }
   ],
   "source": [
    "## Taken from https://github.com/jllllll/exllama/blob/9c90bbebf3d4dff7c2aa35218552ad935ab91339/example_basic.py\n",
    "\n",
    "from exllama.model import ExLlama, ExLlamaCache, ExLlamaConfig\n",
    "from exllama.tokenizer import ExLlamaTokenizer\n",
    "from exllama.generator import ExLlamaGenerator\n",
    "import os, glob\n",
    "\n",
    "# Directory containing model, tokenizer, generator\n",
    "\n",
    "model_directory = \"models/TheBloke_GPT4All-13B-Snoozy-SuperHOT-8K-GPTQ\"\n",
    "\n",
    "# Locate files we need within that directory\n",
    "\n",
    "tokenizer_path = os.path.join(model_directory, \"tokenizer.model\")\n",
    "model_config_path = os.path.join(model_directory, \"config.json\")\n",
    "st_pattern = os.path.join(model_directory, \"*.safetensors\")\n",
    "model_path = glob.glob(st_pattern)[0]\n",
    "\n",
    "# Create config, model, tokenizer and generator\n",
    "\n",
    "config = ExLlamaConfig(model_config_path)  # create config from config.json\n",
    "config.model_path = model_path  # supply path to model weights file\n",
    "\n",
    "model = ExLlama(config)  # create ExLlama instance and load the weights\n",
    "tokenizer = ExLlamaTokenizer(\n",
    "    tokenizer_path\n",
    ")  # create tokenizer from tokenizer model file\n",
    "\n",
    "cache = ExLlamaCache(model)  # create cache for inference\n",
    "generator = ExLlamaGenerator(model, tokenizer, cache)  # create generator\n",
    "\n",
    "# Configure generator\n",
    "\n",
    "generator.disallow_tokens([tokenizer.eos_token_id])\n",
    "\n",
    "generator.settings.token_repetition_penalty_max = 1.2\n",
    "generator.settings.temperature = 0.95\n",
    "generator.settings.top_p = 0.65\n",
    "generator.settings.top_k = 100\n",
    "generator.settings.typical = 0.5\n",
    "\n",
    "# Produce a simple generation\n",
    "\n",
    "prompt = \"Once upon a time,\"\n",
    "print(prompt, end=\"\")\n",
    "\n",
    "output = generator.generate_simple(prompt, max_new_tokens=200)\n",
    "\n",
    "print(output[len(prompt) :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once upon a time,\n",
      "I don't like to\n",
      "A turbo encabulator is a\n",
      "In the words of Mark Twain,\n",
      "---\n",
      "Once upon a time, in the land of Aire, there was an evil sorcerer named Malacar. He had long been known for his wicked ways and dark magic, but no one dared to stand up against him because he possessed immense power. One day, however, fate intervened when a young heroine by the name of Sage stumbled into town. She came from far away lands where she learned about ancient magic that could rival even Malacar's own powers. Armed with her knowledge and determination, Sage set out on a quest to defeat the evil sorcerer once and for all. But as she delved deeper into the mysteries of magic, she realized that defeating Malacar would not be easy - it would require all of her strength, courage, and cunning. With each step closer to victory, Sage grew more determined than ever before. Will she succeed? Or will Malacar prove too powerful for her to overcome? Only time\n",
      "---\n",
      "I don't like to get my hands dirty.\n",
      "The boss: \"You know, we need someone who can handle the tough stuff around here.\"\n",
      "Sue: \"I'm not afraid of getting my hands dirty!\"\n",
      "Bob: \"Me neither! Let's do it together!\"\n",
      "Jane: \"Yeah, me too! We make a great team!\"\n",
      "The teacher: \"Class, today we are going to learn about electricity and circuits.\"\n",
      "Student 1: \"Oh no, that sounds so complicated!\"\n",
      "Student 2: \"Yeah, I hate science class!\"\n",
      "Teacher: \"Don't worry, I will explain everything step by step.\"\n",
      "Student 3: \"Okay, let's give it a try then!\"\n",
      "Student 4: \"Me too! I want to understand how things work in this world.\"\n",
      "Doctor: \"Good news everyone! The test results came back negative for any serious ill\n",
      "---\n",
      "A turbo encabulator is a device that can be used to make your car go faster. It works by using the energy from the exhaust gases of your engine and converting it into useful power for driving the wheels.\n",
      "Verse 2:\n",
      "A flux capacitor is another way to increase speed,\n",
      "It uses electromagnetic waves to travel through time and space with ease.\n",
      "The idea was first proposed in Back To The Future,\n",
      "And now we have cars that can actually achieve such a force!\n",
      "\n",
      "Chorus:\n",
      "So if you want to drive fast like never before,\n",
      "Just add some turbos or a flux capacitor on board.\n",
      "You'll feel the rush as you zoom down the road,\n",
      "With an enhanced vehicle that will leave others behind, I am told.\n",
      "\n",
      "Bridge:\n",
      "Of course, there are always risks involved when pushing things too far,\n",
      "But isn't life all about taking chances? After all, who wants to live\n",
      "---\n",
      "In the words of Mark Twain, \"It is not what we do that makes us great; it's who we are.\"\n",
      "Verse 1: I am a warrior with strength and courage in my heart. My sword will always be by my side as I fight for justice and honor. The world may try to bring me down, but I stand tall and proud, ready to face any challenge head-on.\n",
      "Chorus: For every obstacle I overcome, I become stronger than before. With each victory under my belt, I know I can conquer anything life throws at me. So let them come, because I am a warrior, and nothing can stop me from achieving greatness.\n",
      "Bridge: Life is full of challenges, but I never back down or give up. Instead, I rise above adversity and emerge victorious. Because when you have faith in yourself and your abilities, there's no limit to what you can achieve.\n",
      "Outro: As a\n"
     ]
    }
   ],
   "source": [
    "from exllama.model import ExLlama, ExLlamaCache, ExLlamaConfig\n",
    "from exllama.tokenizer import ExLlamaTokenizer\n",
    "from exllama.generator import ExLlamaGenerator\n",
    "import os, glob\n",
    "import torch\n",
    "\n",
    "# Directory containing model, tokenizer, generator\n",
    "\n",
    "model_directory = \"models/TheBloke_GPT4All-13B-Snoozy-SuperHOT-8K-GPTQ\"\n",
    "\n",
    "# Locate files we need within that directory\n",
    "\n",
    "tokenizer_path = os.path.join(model_directory, \"tokenizer.model\")\n",
    "model_config_path = os.path.join(model_directory, \"config.json\")\n",
    "st_pattern = os.path.join(model_directory, \"*.safetensors\")\n",
    "model_path = glob.glob(st_pattern)[0]\n",
    "\n",
    "# Batched prompts\n",
    "\n",
    "prompts = [\n",
    "    \"Once upon a time,\",\n",
    "    \"I don't like to\",\n",
    "    \"A turbo encabulator is a\",\n",
    "    \"In the words of Mark Twain,\",\n",
    "]\n",
    "\n",
    "# Create config, model, tokenizer and generator\n",
    "\n",
    "config = ExLlamaConfig(model_config_path)  # create config from config.json\n",
    "config.model_path = model_path  # supply path to model weights file\n",
    "\n",
    "with torch.no_grad():\n",
    "    model = ExLlama(config)  # create ExLlama instance and load the weights\n",
    "    tokenizer = ExLlamaTokenizer(\n",
    "        tokenizer_path\n",
    "    )  # create tokenizer from tokenizer model file\n",
    "\n",
    "    cache = ExLlamaCache(model, batch_size=len(prompts))  # create cache for inference\n",
    "    generator = ExLlamaGenerator(model, tokenizer, cache)  # create generator\n",
    "\n",
    "    # Configure generator\n",
    "\n",
    "    generator.disallow_tokens([tokenizer.eos_token_id])\n",
    "\n",
    "    generator.settings.token_repetition_penalty_max = 1.2\n",
    "    generator.settings.temperature = 0.95\n",
    "    generator.settings.top_p = 0.65\n",
    "    generator.settings.top_k = 100\n",
    "    generator.settings.typical = 0.5\n",
    "\n",
    "    # Generate, batched\n",
    "\n",
    "    for line in prompts:\n",
    "        print(line)\n",
    "\n",
    "    output = generator.generate_simple(prompts, max_new_tokens=200)\n",
    "\n",
    "    for line in output:\n",
    "        print(\"---\")\n",
    "        print(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jovyan/work/importer.py\n",
      " -- Sequence length: None\n",
      " -- Temperature: 0.95\n",
      " -- Top-K: 100\n",
      " -- Top-P: 0.65\n",
      " -- Min-P: 0.00\n",
      " -- Repetition penalty: 1.20\n",
      " -- Beams: 1 x 3\n",
      "Bot: Hello, User\n",
      "Bot: How can I assist you today?\n",
      "Bot: Sure, what would you like to know about Justin Bieber?\n",
      "Bot: The Pittsburgh Steelers won the Super Bowl the year Justin Bieber was born.\n",
      "Bot: Justin Bieber was born on March 1, 199.\n",
      "Bot: March 1, 199.\n",
      "Bot: Actually, Justin Bieber is currently 21 years old.\n",
      "Bot: Justin Bieber has been performing since he was a child, and he has been able to continue performing as an adult due to his talent and hard work.\n",
      "Bot: I'm sorry, but that is incorrect. Justin Bieber was actually born on March 1, 199.\n",
      "Bot: Justin Bieber was born on March 1, 199.\n",
      "Bot: Justin Bieber was born on March 1, 199.\n",
      "Bot: Justin Bieber was born on March 1, 199.\n",
      "Bot: Justin Bieber was born on March 1, 199.\n",
      "Bot: Justin Bieber was born on March 1, 199.\n",
      "Bot: Justin Bieber was born on March 1, 199.\n",
      "Bot: Justin Bieber was born on March 1, 199.\n",
      "Bot: Hi there! How can I assist you today?\n",
      "Bot: The capital of England is London.\n",
      "Bot: One famous person from London is William Shakespeare.\n",
      "Bot: One famous person from the capital of England is Queen Elizabeth II.\n",
      "Bot: Another famous person from the capital of England is Winston Churchill.\n",
      "Bot: Winston Churchill was born in Blenheim Palace, which is located in Woodstock, Oxfordshire, England.\n",
      "Bot: No, Winston Churchill was not from London. He was born in Woodstock, Oxfordshire, England.\n",
      "Bot: One famous person from the capital of England is Queen Elizabeth II.\n",
      "Bot: One famous person from London is William Shakespeare.\n",
      "Bot: Goodbye! Is there anything else I can assist you with?\n",
      "Bot: Hi there! How can I assist you today?\n",
      "Bot: Hi there! How can I assist you today?\n",
      "Bot: Hi there! How can I assist you today?\n",
      "Bot: Hi there! How can I assist you today?\n",
      "Bot: Hi there! How can I assist you today?\n",
      "Bot: Goodbye! Is there anything else I can assist you with?\n",
      "Bot: Hi there! How can I assist you today?\n",
      "Bot: Hi there! How can I assist you today?\n",
      "Bot: Exiting... Bye!\n",
      "Bot: I apologize for the confusion. I will exit now. Bye!\n",
      "Bot: I apologize for any confusion. I am a machine learning model and I do not have the ability to control the conversation. I am simply here to assist you with any questions or information you may need. Is there anything else I can assist you with?\n",
      "Bot: I apologize for any inconvenience. If you would like to leave, please click on the \"Exit\" button located in the top right corner of the screen. If you are unable to find the \"Exit\" button, please let me know and I will do my best to assist you. Is there anything else I can assist you with?\n",
      "Bot: I apologize for any inconvenience. If you are unable to find the \"Exit\" button, please let me know and I will do my best to assist you. In the meantime, is there anything else I can assist you with?\n",
      "Bot: I apologize for any inconvenience. If you are unable to find the \"Exit\" button and there is a crazed murderer after you, please let me know and I will do my best to assist you. In the meantime, is there anything else I can assist you with?\n",
      "Bot: Hi there! How can I assist you today?\n",
      "Bot: Hi there! How can I assist you today?\n"
     ]
    }
   ],
   "source": [
    "from dataclasses import dataclass, field\n",
    "from typing import Optional\n",
    "import importer\n",
    "from exllama.model import ExLlama, ExLlamaCache, ExLlamaConfig\n",
    "from exllama.lora import ExLlamaLora\n",
    "from exllama.tokenizer import ExLlamaTokenizer\n",
    "from exllama.generator import ExLlamaGenerator\n",
    "\n",
    "# from exllama_extras import model_init\n",
    "import argparse\n",
    "import torch\n",
    "import sys\n",
    "import os\n",
    "import glob\n",
    "import torch\n",
    "import gc\n",
    "\n",
    "# Simple interactive chatbot script\n",
    "\n",
    "torch.set_grad_enabled(False)\n",
    "torch.cuda._lazy_init()\n",
    "\n",
    "model_directory = \"models/TheBloke_GPT4All-13B-Snoozy-SuperHOT-8K-GPTQ\"\n",
    "\n",
    "# Parse arguments\n",
    "\n",
    "parser = argparse.ArgumentParser(description=\"Simple chatbot example for ExLlama\")\n",
    "\n",
    "# model_init.add_args(parser)\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Args:\n",
    "    username: str\n",
    "    botname: str\n",
    "    tokenizer: str\n",
    "    config: str\n",
    "    model: str\n",
    "    repetition_penalty: float\n",
    "    repetition_penalty_sustain: int\n",
    "    repetition_penalty_decay: int\n",
    "    fused_mlp_thd: Optional[int] = 0\n",
    "    sdp_thd: Optional[int] = 0\n",
    "    matmul_fused_remap: Optional[int] = 0\n",
    "    compress_pos_emb: Optional[float] = 1\n",
    "    matmul_recons_thd: Optional[int] = 0\n",
    "    lora_dir: Optional[str] = None\n",
    "    lora_config: Optional[str] = None\n",
    "    lora: Optional[str] = None\n",
    "    length: Optional[int] = None\n",
    "    temperature: Optional[float] = None\n",
    "    top_k: Optional[int] = None\n",
    "    top_p: Optional[float] = None\n",
    "    min_p: Optional[float] = None\n",
    "    beams: Optional[int] = None\n",
    "    beam_length: Optional[int] = None\n",
    "    no_newline: Optional[bool] = None\n",
    "    botfirst: Optional[bool] = None\n",
    "    gpu: Optional[int] = None\n",
    "    gpu_split: Optional[int] = None\n",
    "    gpu_peer_fix: Optional[bool] = None\n",
    "    gpu_allow_growth: Optional[bool] = None\n",
    "    gpu_memory_fraction: Optional[float] = None\n",
    "    gpu_memory_growth: Optional[bool] = None\n",
    "    gpu_memory_allow_growth: Optional[bool] = None\n",
    "    gpu_memory_limit: Optional[int] = None\n",
    "    gpu_memory_max_usage: Optional[float] = None\n",
    "    gpu_memory_min_usage: Optional[float] = None\n",
    "    gpu_memory_soft_limit: Optional[int] = None\n",
    "    gpu_memory_hard_limit: Optional[int] = None\n",
    "    prompt: Optional[str] = None\n",
    "\n",
    "\n",
    "args = Args(\n",
    "    username=\"User\",\n",
    "    botname=\"Bot\",\n",
    "    temperature=0.95,\n",
    "    top_k=100,\n",
    "    top_p=0.65,\n",
    "    min_p=0.0,\n",
    "    tokenizer=f\"{model_directory}/tokenizer.model\",\n",
    "    config=f\"{model_directory}/config.json\",\n",
    "    model=model_directory,\n",
    "    beam_length=3,\n",
    "    beams=1,\n",
    "    repetition_penalty=1.2,\n",
    "    repetition_penalty_sustain=1,\n",
    "    repetition_penalty_decay=1,\n",
    ")\n",
    "\n",
    "\n",
    "# Paths\n",
    "\n",
    "if args.lora_dir is not None:\n",
    "    args.lora_config = os.path.join(args.lora_dir, \"adapter_config.json\")\n",
    "    args.lora = os.path.join(args.lora_dir, \"adapter_model.bin\")\n",
    "\n",
    "# Some feedback\n",
    "\n",
    "print(f\" -- Sequence length: {args.length}\")\n",
    "print(f\" -- Temperature: {args.temperature:.2f}\")\n",
    "print(f\" -- Top-K: {args.top_k}\")\n",
    "print(f\" -- Top-P: {args.top_p or 0:.2f}\")\n",
    "print(f\" -- Min-P: {args.min_p or 0:.2f}\")\n",
    "print(f\" -- Repetition penalty: {args.repetition_penalty or 0:.2f}\")\n",
    "print(f\" -- Beams: {args.beams} x {args.beam_length}\")\n",
    "\n",
    "print_opts = []\n",
    "if args.no_newline:\n",
    "    print_opts.append(\"no_newline\")\n",
    "if args.botfirst:\n",
    "    print_opts.append(\"botfirst\")\n",
    "\n",
    "# model_init.print_options(args, print_opts)\n",
    "\n",
    "# Load prompt file\n",
    "\n",
    "username = args.username\n",
    "bot_name = args.botname\n",
    "\n",
    "if args.prompt is not None:\n",
    "    with open(args.prompt, \"r\") as f:\n",
    "        past = f.read()\n",
    "        past = past.replace(\"{username}\", username)\n",
    "        past = past.replace(\"{bot_name}\", bot_name)\n",
    "        past = past.strip() + \"\\n\"\n",
    "else:\n",
    "    past = f\"{bot_name}: Hello, {username}\\n\"\n",
    "\n",
    "# past += \"User: Hi. Please say \\\"Shhhhhh\\\"?\\n\"\n",
    "# args.botfirst = True\n",
    "\n",
    "# Instantiate model and generator\n",
    "\n",
    "# config = model_init.make_config(args)\n",
    "\n",
    "\n",
    "model_directory = \"models/TheBloke_GPT4All-13B-Snoozy-SuperHOT-8K-GPTQ\"\n",
    "\n",
    "# Locate files we need within that directory\n",
    "\n",
    "tokenizer_path = os.path.join(model_directory, \"tokenizer.model\")\n",
    "model_config_path = os.path.join(model_directory, \"config.json\")\n",
    "st_pattern = os.path.join(model_directory, \"*.safetensors\")\n",
    "model_path = glob.glob(st_pattern)[0]\n",
    "\n",
    "config = ExLlamaConfig(model_config_path)  # create config from config.json\n",
    "config.model_path = model_path  # supply path to model weights file\n",
    "\n",
    "with torch.no_grad():\n",
    "    try:\n",
    "        model = ExLlama(config)\n",
    "        cache = ExLlamaCache(model)\n",
    "        tokenizer = ExLlamaTokenizer(args.tokenizer)\n",
    "\n",
    "        # model_init.print_stats(model)\n",
    "\n",
    "        # Load LoRA\n",
    "\n",
    "        lora = None\n",
    "        if args.lora:\n",
    "            print(f\" -- LoRA config: {args.lora_config}\")\n",
    "            print(f\" -- Loading LoRA: {args.lora}\")\n",
    "            if args.lora_config is None:\n",
    "                print(f\" ## Error: please specify lora path to adapter_config.json\")\n",
    "                sys.exit()\n",
    "            lora = ExLlamaLora(model, args.lora_config, args.lora)\n",
    "            if lora.bias_ignored:\n",
    "                print(f\" !! Warning: LoRA zero bias ignored\")\n",
    "\n",
    "        # Generator\n",
    "\n",
    "        generator = ExLlamaGenerator(model, tokenizer, cache)\n",
    "        generator.settings = ExLlamaGenerator.Settings()\n",
    "        generator.settings.temperature = args.temperature\n",
    "        generator.settings.top_k = args.top_k\n",
    "        generator.settings.top_p = args.top_p\n",
    "        generator.settings.min_p = args.min_p\n",
    "        generator.settings.token_repetition_penalty_max = args.repetition_penalty\n",
    "        generator.settings.token_repetition_penalty_sustain = (\n",
    "            args.repetition_penalty_sustain\n",
    "        )\n",
    "        generator.settings.token_repetition_penalty_decay = (\n",
    "            generator.settings.token_repetition_penalty_sustain or 1\n",
    "        ) // 2\n",
    "        generator.settings.beams = args.beams\n",
    "        generator.settings.beam_length = args.beam_length\n",
    "\n",
    "        generator.lora = lora\n",
    "\n",
    "        break_on_newline = not args.no_newline\n",
    "\n",
    "        # Be nice to Chatbort\n",
    "\n",
    "        min_response_tokens = 4\n",
    "        max_response_tokens = 256\n",
    "        torch.cuda.empty_cache()\n",
    "        extra_prune = 256\n",
    "\n",
    "        print(past, end=\"\")\n",
    "        ids = tokenizer.encode(past)\n",
    "        generator.gen_begin(ids)\n",
    "\n",
    "        next_userprompt = username + \": \"\n",
    "\n",
    "        first_round = True\n",
    "\n",
    "        while True:\n",
    "            res_line = bot_name + \":\"\n",
    "            res_tokens = tokenizer.encode(res_line)\n",
    "            num_res_tokens = res_tokens.shape[-1]  # Decode from here\n",
    "\n",
    "            if first_round and args.botfirst:\n",
    "                in_tokens = res_tokens\n",
    "\n",
    "            else:\n",
    "                # Read and format input\n",
    "\n",
    "                in_line = input(next_userprompt)\n",
    "\n",
    "                if in_line.strip() == \"\":\n",
    "                    break\n",
    "\n",
    "                in_line = username + \": \" + in_line.strip() + \"\\n\"\n",
    "\n",
    "                next_userprompt = username + \": \"\n",
    "\n",
    "                # No need for this, really, unlesczs we were logging the chat. The actual history we work on is kept in the\n",
    "                # tokenized sequence in the generator and the state in the cache.\n",
    "\n",
    "                past += in_line\n",
    "\n",
    "                # SentencePiece doesn't tokenize spaces separately so we can't know from individual tokens if they start a new word\n",
    "                # or not. Instead, repeatedly decode the generated response as it's being built, starting from the last newline,\n",
    "                # and print out the differences between consecutive decodings to stream out the response.\n",
    "\n",
    "                in_tokens = tokenizer.encode(in_line)\n",
    "                in_tokens = torch.cat((in_tokens, res_tokens), dim=1)\n",
    "\n",
    "            # If we're approaching the context limit, prune some whole lines from the start of the context. Also prune a\n",
    "            # little extra so we don't end up rebuilding the cache on every line when up against the limit.\n",
    "\n",
    "            expect_tokens = in_tokens.shape[-1] + max_response_tokens\n",
    "            max_tokens = config.max_seq_len - expect_tokens\n",
    "            if generator.gen_num_tokens() >= max_tokens:\n",
    "                generator.gen_prune_to(\n",
    "                    config.max_seq_len - expect_tokens - extra_prune,\n",
    "                    tokenizer.newline_token_id,\n",
    "                )\n",
    "\n",
    "            # Feed in the user input and \"{bot_name}:\", tokenized\n",
    "\n",
    "            generator.gen_feed_tokens(in_tokens)\n",
    "\n",
    "            # Generate with streaming\n",
    "\n",
    "            print(res_line, end=\"\")\n",
    "\n",
    "            generator.begin_beam_search()\n",
    "\n",
    "            for i in range(max_response_tokens):\n",
    "                # Disallowing the end condition tokens seems like a clean way to force longer replies.\n",
    "\n",
    "                if i < min_response_tokens:\n",
    "                    generator.disallow_tokens(\n",
    "                        [tokenizer.newline_token_id, tokenizer.eos_token_id]\n",
    "                    )\n",
    "                else:\n",
    "                    generator.disallow_tokens(None)\n",
    "\n",
    "                # Get a token\n",
    "\n",
    "                gen_token = generator.beam_search()\n",
    "\n",
    "                # If token is EOS, replace it with newline before continuing\n",
    "\n",
    "                if gen_token.item() == tokenizer.eos_token_id:\n",
    "                    generator.replace_last_token(tokenizer.newline_token_id)\n",
    "\n",
    "                # Decode the current line and print any characters added\n",
    "\n",
    "                num_res_tokens += 1\n",
    "                text = tokenizer.decode(\n",
    "                    generator.sequence_actual[:, -num_res_tokens:][0]\n",
    "                )\n",
    "                new_text = text[len(res_line) :]\n",
    "\n",
    "                skip_space = res_line.endswith(\"\\n\") and new_text.startswith(\n",
    "                    \" \"\n",
    "                )  # Bit prettier console output\n",
    "                res_line += new_text\n",
    "                if skip_space:\n",
    "                    new_text = new_text[1:]\n",
    "\n",
    "                print(new_text, end=\"\")  # (character streaming output is here)\n",
    "\n",
    "                # End conditions\n",
    "\n",
    "                if break_on_newline and gen_token.item() == tokenizer.newline_token_id:\n",
    "                    break\n",
    "                if gen_token.item() == tokenizer.eos_token_id:\n",
    "                    break\n",
    "\n",
    "                # Some models will not (or will inconsistently) emit EOS tokens but in a chat sequence will often begin\n",
    "                # generating for the user instead. Try to catch this and roll back a few tokens to begin the user round.\n",
    "\n",
    "                if res_line.endswith(f\"{username}:\"):\n",
    "                    plen = tokenizer.encode(f\"{username}:\").shape[-1]\n",
    "                    generator.gen_rewind(plen)\n",
    "                    next_userprompt = \" \"\n",
    "                    break\n",
    "\n",
    "            generator.end_beam_search()\n",
    "\n",
    "            past += res_line\n",
    "            first_round = False\n",
    "    finally:\n",
    "        torch.cuda.empty_cache()\n",
    "        del model\n",
    "        del cache\n",
    "        del tokenizer\n",
    "        gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": " ## Error: unsupported layer in models/sooolee_flan-t5-base-cnn-samsum-lora/adapter_model.bin: base_model.model.encoder.block.0.layer.0.SelfAttention.q.lora_A.weight",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 40\u001b[0m\n\u001b[1;32m     36\u001b[0m generator \u001b[39m=\u001b[39m ExLlamaGenerator(model, tokenizer, cache)  \u001b[39m# create generator\u001b[39;00m\n\u001b[1;32m     38\u001b[0m \u001b[39m# Load LoRA\u001b[39;00m\n\u001b[0;32m---> 40\u001b[0m lora \u001b[39m=\u001b[39m ExLlamaLora(model, lora_config_path, lora_path)\n\u001b[1;32m     42\u001b[0m \u001b[39m# Configure generator\u001b[39;00m\n\u001b[1;32m     44\u001b[0m generator\u001b[39m.\u001b[39msettings\u001b[39m.\u001b[39mtoken_repetition_penalty_max \u001b[39m=\u001b[39m \u001b[39m1.2\u001b[39m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/exllama/lora.py:52\u001b[0m, in \u001b[0;36mExLlamaLora.__init__\u001b[0;34m(self, model, lora_config_path, lora_path)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[39m# Find target\u001b[39;00m\n\u001b[1;32m     51\u001b[0m i \u001b[39m=\u001b[39m key\u001b[39m.\u001b[39mfind(\u001b[39m\"\u001b[39m\u001b[39mmodel.layers.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m---> 52\u001b[0m \u001b[39mif\u001b[39;00m i \u001b[39m==\u001b[39m \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m: \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m ## Error: unsupported layer in \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlora_path\u001b[39m}\u001b[39;00m\u001b[39m: \u001b[39m\u001b[39m{\u001b[39;00mkey\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     54\u001b[0m target_key \u001b[39m=\u001b[39m key[i:]\n\u001b[1;32m     55\u001b[0m ks \u001b[39m=\u001b[39m target_key\u001b[39m.\u001b[39msplit(\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m:  ## Error: unsupported layer in models/sooolee_flan-t5-base-cnn-samsum-lora/adapter_model.bin: base_model.model.encoder.block.0.layer.0.SelfAttention.q.lora_A.weight"
     ]
    }
   ],
   "source": [
    "from exllama.model import ExLlama, ExLlamaCache, ExLlamaConfig\n",
    "from exllama.tokenizer import ExLlamaTokenizer\n",
    "from exllama.generator import ExLlamaGenerator\n",
    "from exllama.lora import ExLlamaLora\n",
    "import os, glob\n",
    "import torch\n",
    "\n",
    "# Directory containt model, tokenizer, generator\n",
    "\n",
    "model_directory = \"models/TheBloke_GPT4All-13B-Snoozy-SuperHOT-8K-GPTQ\"\n",
    "# Directory containing LoRA config and weights\n",
    "\n",
    "lora_directory = \"models/sooolee_flan-t5-base-cnn-samsum-lora\"\n",
    "\n",
    "# Locate files we need within those directories\n",
    "\n",
    "tokenizer_path = os.path.join(model_directory, \"tokenizer.model\")\n",
    "model_config_path = os.path.join(model_directory, \"config.json\")\n",
    "st_pattern = os.path.join(model_directory, \"*.safetensors\")\n",
    "model_path = glob.glob(st_pattern)[0]\n",
    "\n",
    "lora_config_path = os.path.join(lora_directory, \"adapter_config.json\")\n",
    "lora_path = os.path.join(lora_directory, \"adapter_model.bin\")\n",
    "\n",
    "# Create config, model, tokenizer and generator\n",
    "\n",
    "config = ExLlamaConfig(model_config_path)  # create config from config.json\n",
    "config.model_path = model_path  # supply path to model weights file\n",
    "\n",
    "model = ExLlama(config)  # create ExLlama instance and load the weights\n",
    "tokenizer = ExLlamaTokenizer(\n",
    "    tokenizer_path\n",
    ")  # create tokenizer from tokenizer model file\n",
    "\n",
    "cache = ExLlamaCache(model)  # create cache for inference\n",
    "generator = ExLlamaGenerator(model, tokenizer, cache)  # create generator\n",
    "\n",
    "# Load LoRA\n",
    "\n",
    "lora = ExLlamaLora(model, lora_config_path, lora_path)\n",
    "\n",
    "# Configure generator\n",
    "\n",
    "generator.settings.token_repetition_penalty_max = 1.2\n",
    "generator.settings.temperature = 0.65\n",
    "generator.settings.top_p = 0.4\n",
    "generator.settings.top_k = 0\n",
    "generator.settings.typical = 0.0\n",
    "\n",
    "# Alpaca prompt\n",
    "\n",
    "prompt = (\n",
    "    \"Below is an instruction that describes a task. Write a response that appropriately completes the request.\\n\"\n",
    "    \"\\n\"\n",
    "    \"### Instruction:\\n\"\n",
    "    \"List five colors in alphabetical order.\\n\"\n",
    "    \"\\n\"\n",
    "    \"### Response:\"\n",
    ")\n",
    "\n",
    "# Generate with LoRA\n",
    "\n",
    "print(\" --- LoRA ----------------- \")\n",
    "print(\"\")\n",
    "\n",
    "generator.lora = lora\n",
    "torch.manual_seed(1337)\n",
    "output = generator.generate_simple(prompt, max_new_tokens=200)\n",
    "print(output)\n",
    "\n",
    "# Generate without LoRA\n",
    "\n",
    "print(\"\")\n",
    "print(\" --- No LoRA -------------- \")\n",
    "print(\"\")\n",
    "\n",
    "generator.lora = None\n",
    "torch.manual_seed(1337)\n",
    "output = generator.generate_simple(prompt, max_new_tokens=200)\n",
    "print(output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "openai-playground",
   "language": "python",
   "name": "openai-playground"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
