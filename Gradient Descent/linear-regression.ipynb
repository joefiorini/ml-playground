{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> https://chat.openai.com/share/1a8d2884-7cf4-4f26-a3b6-87fdb254d492\n",
    "\n",
    "**Objective:** The goal of this lab is to reinforce your understanding of gradient descent by implementing it from scratch to solve a simple linear regression problem. You will be using Python, Jupyter notebooks, and PyTorch for this assignment.\n",
    "\n",
    "**Problem Description:** You are given a dataset with two variables, x and y, where y is a linear function of x with some added noise. Your task is to find the line of best fit for this data using gradient descent.\n",
    "\n",
    "**Instructions:**\n",
    "\n",
    "1. **Data Generation:** Generate a synthetic dataset of 100 points for this task. You can use the torch.rand function to generate x and then define y as y = 2x + 3 + noise, where noise is a random value added to each point to simulate real-world data. The coefficients 2 and 3 are the true weight and bias, respectively, that you will try to learn with gradient descent.\n",
    "\n",
    "2. **Model Definition:** Define a simple linear regression model y = wx + b, where w is the weight and b is the bias. Initialize w and b to any values of your choice.\n",
    "\n",
    "3. **Loss Function:** Define the mean squared error loss function, which is the function you will minimize using gradient descent.\n",
    "\n",
    "4. **Gradient Descent:** Implement the gradient descent algorithm. At each step, compute the gradients of the loss with respect to w and b, and then update w and b in the direction that reduces the loss. Repeat this process for a fixed number of iterations, or until w and b converge to the true values within a certain tolerance.\n",
    "\n",
    "5. **Evaluation:** Plot the original data along with the line of best fit found by your model. Also, plot the loss over time to see how it decreases as the model learns.\n",
    "\n",
    "**Questions to Consider:**\n",
    "\n",
    "- How do different initial values of w and b affect the number of iterations needed for convergence?\n",
    "- How does the learning rate affect the speed of convergence and the final result?\n",
    "- What happens if the learning rate is set too high or too low?\n",
    "- How does the model perform if you increase the amount of noise in the data?\n",
    "- Remember, you can ask for help at any time if you're unsure about how to proceed. Good luck!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.testing import assert_close\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "xs = torch.randn(100, 1) * 10\n",
    "noise = torch.randn(100, 1) * 3\n",
    "ys = torch.nn.functional.linear(xs, torch.tensor([[2.0]]), torch.tensor([3.0]) + noise)\n",
    "\n",
    "\n",
    "assert_close(ys, 2 * xs + 3 + noise)\n",
    "kak"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([100, 1])\n",
      "torch.Size([1, 1])\n",
      "torch.Size([1])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "assigned grad has data of a different size",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[43], line 29\u001b[0m\n\u001b[1;32m     27\u001b[0m             b\u001b[39m.\u001b[39mgrad\u001b[39m.\u001b[39mzero_()\n\u001b[1;32m     28\u001b[0m     \u001b[39mreturn\u001b[39;00m w, b\n\u001b[0;32m---> 29\u001b[0m linear_regression(xs, ys, epochs\u001b[39m=\u001b[39;49m\u001b[39m100\u001b[39;49m, learning_rate\u001b[39m=\u001b[39;49m\u001b[39m0.001\u001b[39;49m)\n",
      "Cell \u001b[0;32mIn[43], line 22\u001b[0m, in \u001b[0;36mlinear_regression\u001b[0;34m(xs, ys, epochs, learning_rate)\u001b[0m\n\u001b[1;32m     20\u001b[0m y_pred \u001b[39m=\u001b[39m \u001b[39m2\u001b[39m \u001b[39m*\u001b[39m xs \u001b[39m+\u001b[39m \u001b[39m3\u001b[39m \u001b[39m+\u001b[39m noise\n\u001b[1;32m     21\u001b[0m loss \u001b[39m=\u001b[39m mean_squared_loss(y_pred, ys)\n\u001b[0;32m---> 22\u001b[0m loss\u001b[39m.\u001b[39;49mgrad \u001b[39m=\u001b[39m compute_gradient(loss, [w, b])\n\u001b[1;32m     23\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[1;32m     24\u001b[0m     w \u001b[39m-\u001b[39m\u001b[39m=\u001b[39m learning_rate \u001b[39m*\u001b[39m w\u001b[39m.\u001b[39mgrad\n",
      "\u001b[0;31mRuntimeError\u001b[0m: assigned grad has data of a different size"
     ]
    }
   ],
   "source": [
    "initial_weight = 1.0\n",
    "initial_bias = 1.0\n",
    "\n",
    "def mean_squared_loss(y_pred, y_true):\n",
    "    return ((y_pred - y_true) ** 2).mean()\n",
    "\n",
    "def compute_gradient(loss, params):\n",
    "    # Compute the gradient of loss with respect to params\n",
    "    # For y=wx+b (using mean-squared loss function) the gradient is:\n",
    "    # dw = 2*x*(wx+b-y)\n",
    "    return 2 * xs * (params[0] * xs + params[1] - ys)\n",
    "\n",
    "def linear_regression(xs, ys, epochs, learning_rate):\n",
    "    w = torch.tensor([[initial_weight]], dtype=torch.float32, requires_grad=False)\n",
    "    b = torch.tensor([initial_bias], dtype=torch.float32, requires_grad=False)\n",
    "    print(xs.shape)\n",
    "    print(w.shape)\n",
    "    print(b.shape)\n",
    "    for epoch in range(epochs):\n",
    "        y_pred = 2 * xs + 3 + noise\n",
    "        loss = mean_squared_loss(y_pred, ys)\n",
    "        loss.grad = compute_gradient(loss, [w, b])\n",
    "        with torch.no_grad():\n",
    "            w -= learning_rate * w.grad\n",
    "            b -= learning_rate * b.grad\n",
    "            w.grad.zero_()\n",
    "            b.grad.zero_()\n",
    "    return w, b\n",
    "linear_regression(xs, ys, epochs=100, learning_rate=0.001)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
